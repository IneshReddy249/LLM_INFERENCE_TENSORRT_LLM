# âš¡ LLM Inference Optimization â€” Qwen2.5-7B + TensorRT-LLM (A100)

<p align="center">
  <img src="assets/trtllm-setup.png" alt="TRT-LLM Setup" width="720">
</p>

<p align="center">
  <b>ğŸš€ End-to-end benchmark and optimization pipeline for high-performance LLM inference on NVIDIA A100 GPUs.</b><br>
  <i>Compare baseline Hugging Face inference with TensorRT-LLM optimized engines to measure latency, throughput, and GPU efficiency.</i>
</p>

---

## ğŸŒŸ Highlights

- âš™ï¸ **Fully reproducible** TensorRT-LLM workflow: model conversion â†’ engine build â†’ benchmark  
- ğŸš€ **Up to 6Ã— faster** inference vs baseline Hugging Face runtime  
- ğŸ“Š Measures **TTFT**, **latency**, **TPS**, and **GPU utilization**  
- ğŸ”¥ Built for **Qwen2.5-7B-Instruct**, optimized on **NVIDIA A100 (80 GB)**  
- ğŸ§© Integrates **FP16 precision**, **paged KV cache**, **context FMHA**, and **inflight batching**  
- ğŸ³ 100% containerized using **NVIDIA TensorRT-LLM Docker image**

---

## ğŸš€ Overview

**Goal:**  
To reduce inference latency and maximize throughput for large-language models (LLMs) using TensorRT-LLMâ€™s GPU-level optimizations.

**Model:**  
[`Qwen2.5-7B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)

**Hardware:**  
NVIDIA A100 80 GB GPU Â· CUDA 12.9 Â· TensorRT-LLM v0.20.0

---

## ğŸ§± Folder Structure

```text
/workspace/llm-trtllm/
â”œâ”€â”€ hf_models/                     # Hugging Face raw model
â”‚   â””â”€â”€ qwen2.5-7b-instruct/
â”œâ”€â”€ checkpoints/                   # TensorRT-LLM converted weights
â”‚   â””â”€â”€ qwen2.5-7b/
â”œâ”€â”€ engine/                        # Serialized TensorRT engines
â”‚   â”œâ”€â”€ qwen2.5-7b-a100-fp16-lat/
â”‚   â””â”€â”€ qwen2.5-7b-a100-fp16-inflight/
â”œâ”€â”€ results/                       # Benchmark JSON outputs
â”œâ”€â”€ scripts/                       # Conversion / build / benchmark scripts
â”‚   â”œâ”€â”€ 01_convert_qwen.sh
â”‚   â”œâ”€â”€ 02_build_fp16_lat_a100.sh
â”‚   â”œâ”€â”€ 03_build_fp16_inflight_a100.sh
â”‚   â””â”€â”€ benchmark.py
â””â”€â”€ .gitignore
