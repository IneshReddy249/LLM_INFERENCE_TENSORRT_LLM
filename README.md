# âš¡ LLM Inference Optimization â€” Qwen2.5-7B + TensorRT-LLM (A100)

A complete, reproducible benchmark pipeline comparing **baseline Hugging Face inference** vs **TensorRT-LLM optimized inference** on NVIDIA A100 GPUs.  
This project demonstrates how to build TensorRT engines, measure inference efficiency, and analyze metrics such as **latency**, **TTFT**, **throughput (TPS)**, and **GPU utilization**.

---

## ğŸš€ Overview

**Goal:**  
Reduce inference latency and increase throughput for large-language models (LLMs) using **TensorRT-LLM** optimizations such as FP16 precision, paged KV-cache, inflight batching, and context FMHA.

**Model:**  
[`Qwen2.5-7B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)

**Hardware:**  
NVIDIA A100 80 GB GPUâ€ƒâ€¢â€ƒCUDA 12.9â€ƒâ€¢â€ƒTensorRT-LLM v0.20.0

---

## ğŸ§± Folder Structure

```text
/workspace/llm-trtllm/
â”œâ”€â”€ hf_models/                     # Hugging Face raw model
â”‚   â””â”€â”€ qwen2.5-7b-instruct/
â”œâ”€â”€ checkpoints/                   # TRT-LLM converted weights
â”‚   â””â”€â”€ qwen2.5-7b/
â”œâ”€â”€ engine/                        # TensorRT serialized engines
â”‚   â”œâ”€â”€ qwen2.5-7b-a100-fp16-lat/
â”‚   â””â”€â”€ qwen2.5-7b-a100-fp16-inflight/
â”œâ”€â”€ results/                       # Benchmark JSON outputs
â”œâ”€â”€ scripts/                       # Conversion / build / benchmark scripts
â”‚   â”œâ”€â”€ 01_convert_qwen.sh
â”‚   â”œâ”€â”€ 02_build_fp16_lat_a100.sh
â”‚   â”œâ”€â”€ 03_build_fp16_inflight_a100.sh
â”‚   â””â”€â”€ benchmark.py
â””â”€â”€ .gitignore


## âš™ï¸ Environment Setup
1ï¸âƒ£ Start Container

docker run --gpus all -it --rm \
  --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \
  -v /home/shadeform/projects/llm-trtllm:/workspace/llm-trtllm \
  nvcr.io/nvidia/tensorrt-llm/release:latest

2ï¸âƒ£ Install Dependencies

cd /workspace/llm-trtllm
pip install --upgrade huggingface_hub transformers accelerate

ğŸ“¥ Download Model Weights

from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="Qwen/Qwen2.5-7B-Instruct",
    local_dir="hf_models/qwen2.5-7b-instruct",
    local_dir_use_symlinks=False
)

ğŸ”„ Convert HF â†’ TRT-LLM Checkpoints

bash scripts/01_convert_qwen.sh
âœ… Outputs to /workspace/llm-trtllm/checkpoints/qwen2.5-7b/

âš¡ Build Optimized Engines (A100)
Latency-optimized (batch = 1)
bash scripts/02_build_fp16_lat_a100.sh

Inflight-batching (batch = 8)
bash scripts/03_build_fp16_inflight_a100.sh

ğŸ§  Benchmark Run
PYTHONPATH="" python3 scripts/benchmark.py

The benchmark measures:

-TTFT (Time to First Token)
-Total Latency
-Throughput (Tokens per Second)
-Output Validation

Results saved to:
results/latency.json

Example Output

{
  "prompt": "Explain paginated KV cache in transformers in simple
English.",
  "tokens": 250,
  "ttft_s": 0.046,
  "latency_s": 1.21,
  "tps": 168.7
}

ğŸ“Š Key Metrics

| Metric         | Description                   | Tool                  |
| :------------- | :---------------------------- | :-------------------- |
| **TTFT**       | Time to first generated token | Benchmark script      |
| **Latency**    | Total generation time         | Benchmark script      |
| **TPS**        | Tokens per second             | Benchmark script      |
| **VRAM Usage** | GPU memory consumption        | `nvidia-smi`          |
| **GPU Util %** | Compute efficiency            | `nvidia-smi --loop=1` |


ğŸ§© Optimization Summary

| Optimization      | Benefit                           | Verified |
| :---------------- | :-------------------------------- | :------: |
| FP16 precision    | Reduced compute & memory load     |     âœ…    |
| Paged KV Cache    | Lower fragmentation, better reuse |     âœ…    |
| Context FMHA      | Faster attention kernel           |     âœ…    |
| Inflight Batching | Higher throughput                 |     âœ…    |
| TensorRT Engine   | Up to 5â€“6Ã— faster vs baseline     |     âœ…    |




